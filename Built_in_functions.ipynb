{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamalakbari7/PySpark/blob/main/Built_in_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qf-TmnsBgXd"
      },
      "source": [
        "# Built-in functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd6t0uFzuR4X"
      },
      "source": [
        "## Download and install Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zToS5PqxBXpK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "343969d5-8589-4e57-e8cb-a6019818d50a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tt7ZS1_wGgjn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b7a46c1-f205-41f5-99a7-bdb345dc073e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Get:4 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "Hit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1,581 B]\n",
            "Hit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:9 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,111 kB]\n",
            "Hit:11 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:12 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2,277 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,329 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [31.2 kB]\n",
            "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [993 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,033 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,629 kB]\n",
            "Fetched 11.7 MB in 4s (3,086 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEuEkV0BlzAD"
      },
      "source": [
        "## Setup environment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Jan 2023 update]**\n",
        "- Google colab recently upgraded to Python 3.8. Unfortunately this breaks spark 2.3.1. \n",
        "- Please use the code below where we install from the pyspark package instead"
      ],
      "metadata": {
        "id": "qu34sKRJvhq_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "We2yp-o7Hgu9",
        "outputId": "97a0a2cd-413c-4dbb-ed28-0ae2e9a59b36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.9/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317145 sha256=74ea14b651266baf2987ce6e1c3999f7a406a694bcf7f3d3a0b4bbc608e0650a\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/34/a4/159aa12d0a510d5ff7c8f0220abbea42e5d81ecf588c4fd884\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "QdrFO3yrwptm",
        "outputId": "14d94427-e67e-4eab-f30f-1edd4b552ed0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fee301f6b20>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://a56f35b305c7:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sdOOq4twHN1K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "005b0688-3d24-4cbb-af4d-f580c12b2668"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fee301f6b20>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://a56f35b305c7:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.1-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate() \n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ysUlfFrN5OD"
      },
      "source": [
        "## Downloading and preprocessing Chicago's Reported Crime Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FDtw5Hy3N-pV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08c31b6a-6dd9-40c3-fe19-3ff56fc09ab5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-21 00:28:24--  https://data.cityofchicago.org/api/views/ijzp-q8t2/rows.csv?accessType=DOWNLOAD\n",
            "Resolving data.cityofchicago.org (data.cityofchicago.org)... 52.206.68.26, 52.206.140.199, 52.206.140.205\n",
            "Connecting to data.cityofchicago.org (data.cityofchicago.org)|52.206.68.26|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘rows.csv?accessType=DOWNLOAD’\n",
            "\n",
            "rows.csv?accessType     [ <=>                ]   1.71G  3.02MB/s    in 9m 44s  \n",
            "\n",
            "2023-04-21 00:38:10 (3.00 MB/s) - ‘rows.csv?accessType=DOWNLOAD’ saved [1836320309]\n",
            "\n",
            "total 2013888\n",
            "-rw-r--r--  1 root root 1836320309 Apr 20 10:56 'rows.csv?accessType=DOWNLOAD'\n",
            "drwxr-xr-x  1 root root       4096 Apr 19 13:37  sample_data\n",
            "drwxrwxr-x 13 1000 1000       4096 Jun  1  2018  spark-2.3.1-bin-hadoop2.7\n",
            "-rw-r--r--  1 root root  225883783 Jun  1  2018  spark-2.3.1-bin-hadoop2.7.tgz\n"
          ]
        }
      ],
      "source": [
        "!wget https://data.cityofchicago.org/api/views/ijzp-q8t2/rows.csv?accessType=DOWNLOAD\n",
        "!ls -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "v4P5mMONYyVd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2d073ad-104e-4b87-f28f-bf5eced2788e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 2013888\n",
            "-rw-r--r--  1 root root 1836320309 Apr 20 10:56 reported-crimes.csv\n",
            "drwxr-xr-x  1 root root       4096 Apr 19 13:37 sample_data\n",
            "drwxrwxr-x 13 1000 1000       4096 Jun  1  2018 spark-2.3.1-bin-hadoop2.7\n",
            "-rw-r--r--  1 root root  225883783 Jun  1  2018 spark-2.3.1-bin-hadoop2.7.tgz\n"
          ]
        }
      ],
      "source": [
        "!mv rows.csv\\?accessType\\=DOWNLOAD reported-crimes.csv\n",
        "!ls -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9-1cK0nPNS95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbbb6c08-788c-446f-e842-85f9a71a651c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+-------------------+--------------------+----+------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
            "|      ID|Case Number|               Date|               Block|IUCR|Primary Type|         Description|Location Description|Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|Year|          Updated On|    Latitude|    Longitude|            Location|\n",
            "+--------+-----------+-------------------+--------------------+----+------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
            "|10224738|   HY411648|2015-09-05 13:30:00|     043XX S WOOD ST|0486|     BATTERY|DOMESTIC BATTERY ...|           RESIDENCE| false|    true|0924|     009|  12|            61|     08B|     1165074|     1875917|2015|02/10/2018 03:50:...|41.815117282|-87.669999562|(41.815117282, -8...|\n",
            "|10224739|   HY411615|2015-09-04 11:30:00| 008XX N CENTRAL AVE|0870|       THEFT|      POCKET-PICKING|             CTA BUS| false|   false|1511|     015|  29|            25|      06|     1138875|     1904869|2015|02/10/2018 03:50:...|41.895080471|-87.765400451|(41.895080471, -8...|\n",
            "|11646166|   JC213529|2018-09-01 00:01:00|082XX S INGLESIDE...|0810|       THEFT|           OVER $500|           RESIDENCE| false|    true|0631|     006|   8|            44|      06|        null|        null|2018|04/06/2019 04:04:...|        null|         null|                null|\n",
            "|10224740|   HY411595|2015-09-05 12:45:00|   035XX W BARRY AVE|2023|   NARCOTICS|POSS: HEROIN(BRN/...|            SIDEWALK|  true|   false|1412|     014|  35|            21|      18|     1152037|     1920384|2015|02/10/2018 03:50:...|41.937405765|-87.716649687|(41.937405765, -8...|\n",
            "|10224741|   HY411610|2015-09-05 13:00:00| 0000X N LARAMIE AVE|0560|     ASSAULT|              SIMPLE|           APARTMENT| false|    true|1522|     015|  28|            25|     08A|     1141706|     1900086|2015|02/10/2018 03:50:...|41.881903443|-87.755121152|(41.881903443, -8...|\n",
            "+--------+-----------+-------------------+--------------------+----+------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import to_timestamp,col,lit\n",
        "rc = spark.read.csv('reported-crimes.csv',header=True).withColumn('Date',to_timestamp(col('Date'),'MM/dd/yyyy hh:mm:ss a')).filter(col('Date') < lit('2018-11-12'))\n",
        "rc.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI_zYOVpf2yK"
      },
      "source": [
        "## Built-in functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "808f1HzwgDyh"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PeeOFy5cgDRq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a232af0c-0df2-4869-ffc9-04b65001b1d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Any', 'ArrayType', 'Callable', 'Column', 'DataFrame', 'DataType', 'Dict', 'Iterable', 'JVMView', 'List', 'Optional', 'PandasUDFType', 'PySparkTypeError', 'PySparkValueError', 'PythonEvalType', 'SparkContext', 'StringType', 'StructType', 'TYPE_CHECKING', 'Tuple', 'Union', 'UserDefinedFunction', 'ValuesView', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_create_column_from_literal', '_create_lambda', '_create_py_udf', '_from_numpy_type', '_get_jvm_function', '_get_lambda_parameters', '_invoke_binary_math_function', '_invoke_function', '_invoke_function_over_columns', '_invoke_function_over_seq_of_columns', '_invoke_higher_order_function', '_options_to_str', '_test', '_to_java_column', '_to_seq', '_unresolved_named_lambda_variable', 'abs', 'acos', 'acosh', 'add_months', 'aggregate', 'approxCountDistinct', 'approx_count_distinct', 'array', 'array_append', 'array_compact', 'array_contains', 'array_distinct', 'array_except', 'array_insert', 'array_intersect', 'array_join', 'array_max', 'array_min', 'array_position', 'array_remove', 'array_repeat', 'array_sort', 'array_union', 'arrays_overlap', 'arrays_zip', 'asc', 'asc_nulls_first', 'asc_nulls_last', 'ascii', 'asin', 'asinh', 'assert_true', 'atan', 'atan2', 'atanh', 'avg', 'base64', 'bin', 'bit_length', 'bitwiseNOT', 'bitwise_not', 'broadcast', 'bround', 'bucket', 'call_udf', 'cast', 'cbrt', 'ceil', 'coalesce', 'col', 'collect_list', 'collect_set', 'column', 'concat', 'concat_ws', 'conv', 'corr', 'cos', 'cosh', 'cot', 'count', 'countDistinct', 'count_distinct', 'covar_pop', 'covar_samp', 'crc32', 'create_map', 'csc', 'cume_dist', 'current_date', 'current_timestamp', 'date_add', 'date_format', 'date_sub', 'date_trunc', 'datediff', 'dayofmonth', 'dayofweek', 'dayofyear', 'days', 'decode', 'degrees', 'dense_rank', 'desc', 'desc_nulls_first', 'desc_nulls_last', 'element_at', 'encode', 'exists', 'exp', 'explode', 'explode_outer', 'expm1', 'expr', 'factorial', 'filter', 'first', 'flatten', 'floor', 'forall', 'format_number', 'format_string', 'from_csv', 'from_json', 'from_unixtime', 'from_utc_timestamp', 'functools', 'get', 'get_active_spark_context', 'get_json_object', 'greatest', 'grouping', 'grouping_id', 'has_numpy', 'hash', 'hex', 'hour', 'hours', 'hypot', 'initcap', 'inline', 'inline_outer', 'input_file_name', 'inspect', 'instr', 'isnan', 'isnull', 'json_tuple', 'kurtosis', 'lag', 'last', 'last_day', 'lead', 'least', 'length', 'levenshtein', 'lit', 'localtimestamp', 'locate', 'log', 'log10', 'log1p', 'log2', 'lower', 'lpad', 'ltrim', 'make_date', 'map_concat', 'map_contains_key', 'map_entries', 'map_filter', 'map_from_arrays', 'map_from_entries', 'map_keys', 'map_values', 'map_zip_with', 'max', 'max_by', 'md5', 'mean', 'median', 'min', 'min_by', 'minute', 'mode', 'monotonically_increasing_id', 'month', 'months', 'months_between', 'nanvl', 'next_day', 'np', 'nth_value', 'ntile', 'octet_length', 'overlay', 'overload', 'pandas_udf', 'percent_rank', 'percentile_approx', 'pmod', 'posexplode', 'posexplode_outer', 'pow', 'product', 'quarter', 'radians', 'raise_error', 'rand', 'randn', 'rank', 'regexp_extract', 'regexp_replace', 'repeat', 'reverse', 'rint', 'round', 'row_number', 'rpad', 'rtrim', 'schema_of_csv', 'schema_of_json', 'sec', 'second', 'sentences', 'sequence', 'session_window', 'sha1', 'sha2', 'shiftLeft', 'shiftRight', 'shiftRightUnsigned', 'shiftleft', 'shiftright', 'shiftrightunsigned', 'shuffle', 'signum', 'sin', 'sinh', 'size', 'skewness', 'slice', 'sort_array', 'soundex', 'spark_partition_id', 'split', 'sqrt', 'stddev', 'stddev_pop', 'stddev_samp', 'struct', 'substring', 'substring_index', 'sum', 'sumDistinct', 'sum_distinct', 'sys', 'tan', 'tanh', 'timestamp_seconds', 'toDegrees', 'toRadians', 'to_csv', 'to_date', 'to_json', 'to_str', 'to_timestamp', 'to_utc_timestamp', 'transform', 'transform_keys', 'transform_values', 'translate', 'trim', 'trunc', 'try_remote_functions', 'udf', 'unbase64', 'unhex', 'unix_timestamp', 'unwrap_udt', 'upper', 'var_pop', 'var_samp', 'variance', 'warnings', 'weekofyear', 'when', 'window', 'window_time', 'xxhash64', 'year', 'years', 'zip_with']\n"
          ]
        }
      ],
      "source": [
        "print(dir(functions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yL-b-swunTM"
      },
      "source": [
        "## String functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHp9A7wyp1L4"
      },
      "source": [
        "**Display the Primary Type column in lower and upper characters, and the first 4 characters of the column**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4HNqGQKCqE9K"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import lower, upper, substring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ugRDxNwNgHNa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f9a8856-0a96-444c-a59a-ab01f7ca22c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function substring in module pyspark.sql.functions:\n",
            "\n",
            "substring(str: 'ColumnOrName', pos: int, len: int) -> pyspark.sql.column.Column\n",
            "    Substring starts at `pos` and is of length `len` when str is String type or\n",
            "    returns the slice of byte array that starts at `pos` in byte and is of length `len`\n",
            "    when str is Binary type.\n",
            "    \n",
            "    .. versionadded:: 1.5.0\n",
            "    \n",
            "    .. versionchanged:: 3.4.0\n",
            "        Supports Spark Connect.\n",
            "    \n",
            "    Notes\n",
            "    -----\n",
            "    The position is not zero based, but 1 based index.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    str : :class:`~pyspark.sql.Column` or str\n",
            "        target column to work on.\n",
            "    pos : int\n",
            "        starting position in str.\n",
            "    len : int\n",
            "        length of chars.\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    :class:`~pyspark.sql.Column`\n",
            "        substring of given value.\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
            "    >>> df.select(substring(df.s, 1, 2).alias('s')).collect()\n",
            "    [Row(s='ab')]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(substring)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rc.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzZCTioQOT-u",
        "outputId": "f5555599-e1df-46a6-ad0a-8ec0c6697573"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- ID: string (nullable = true)\n",
            " |-- Case Number: string (nullable = true)\n",
            " |-- Date: timestamp (nullable = true)\n",
            " |-- Block: string (nullable = true)\n",
            " |-- IUCR: string (nullable = true)\n",
            " |-- Primary Type: string (nullable = true)\n",
            " |-- Description: string (nullable = true)\n",
            " |-- Location Description: string (nullable = true)\n",
            " |-- Arrest: string (nullable = true)\n",
            " |-- Domestic: string (nullable = true)\n",
            " |-- Beat: string (nullable = true)\n",
            " |-- District: string (nullable = true)\n",
            " |-- Ward: string (nullable = true)\n",
            " |-- Community Area: string (nullable = true)\n",
            " |-- FBI Code: string (nullable = true)\n",
            " |-- X Coordinate: string (nullable = true)\n",
            " |-- Y Coordinate: string (nullable = true)\n",
            " |-- Year: string (nullable = true)\n",
            " |-- Updated On: string (nullable = true)\n",
            " |-- Latitude: string (nullable = true)\n",
            " |-- Longitude: string (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rc.select(lower(col('Primary Type')),upper(col('Primary Type')),substring(col('Primary Type'),1,4)).show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuDNpgHoPO1P",
        "outputId": "59214519-6d13-4f18-9bdf-8c2c48279112"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-------------------+-----------------------------+\n",
            "|lower(Primary Type)|upper(Primary Type)|substring(Primary Type, 1, 4)|\n",
            "+-------------------+-------------------+-----------------------------+\n",
            "|            battery|            BATTERY|                         BATT|\n",
            "|              theft|              THEFT|                         THEF|\n",
            "|              theft|              THEFT|                         THEF|\n",
            "|          narcotics|          NARCOTICS|                         NARC|\n",
            "|            assault|            ASSAULT|                         ASSA|\n",
            "+-------------------+-------------------+-----------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYDsLbbrf6dK"
      },
      "source": [
        "## Numeric functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5CZwvAwhpCx"
      },
      "source": [
        "**Show the oldest date and the most recent date**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oP8NWAiUuSJC"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import min,max"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rc.select(min(col('Date')),max(col('Date'))).show(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qexB1swxQOYd",
        "outputId": "5921056c-14d9-460d-a93b-9e645300a1e1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-------------------+\n",
            "|          min(Date)|          max(Date)|\n",
            "+-------------------+-------------------+\n",
            "|2001-01-01 00:00:00|2018-11-11 23:50:00|\n",
            "+-------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkUF4yJFgG25"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BJvThINk7o-"
      },
      "source": [
        "##Date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qjh1t1XamJ-E"
      },
      "source": [
        "** What is 3 days earlier that the oldest date and 3 days later than the most recent date?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "NMFcA4rhlOs3"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import date_add,date_sub "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6_12orCSlXPe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea41d5af-485c-4902-b6ae-acb7a8aefdaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function date_add in module pyspark.sql.functions:\n",
            "\n",
            "date_add(start: 'ColumnOrName', days: Union[ForwardRef('ColumnOrName'), int]) -> pyspark.sql.column.Column\n",
            "    Returns the date that is `days` days after `start`. If `days` is a negative value\n",
            "    then these amount of days will be deducted from `start`.\n",
            "    \n",
            "    .. versionadded:: 1.5.0\n",
            "    \n",
            "    .. versionchanged:: 3.4.0\n",
            "        Supports Spark Connect.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    start : :class:`~pyspark.sql.Column` or str\n",
            "        date column to work on.\n",
            "    days : :class:`~pyspark.sql.Column` or str or int\n",
            "        how many days after the given date to calculate.\n",
            "        Accepts negative value as well to calculate backwards in time.\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    :class:`~pyspark.sql.Column`\n",
            "        a date after/before given number of days.\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> df = spark.createDataFrame([('2015-04-08', 2,)], ['dt', 'add'])\n",
            "    >>> df.select(date_add(df.dt, 1).alias('next_date')).collect()\n",
            "    [Row(next_date=datetime.date(2015, 4, 9))]\n",
            "    >>> df.select(date_add(df.dt, df.add.cast('integer')).alias('next_date')).collect()\n",
            "    [Row(next_date=datetime.date(2015, 4, 10))]\n",
            "    >>> df.select(date_add('dt', -1).alias('prev_date')).collect()\n",
            "    [Row(prev_date=datetime.date(2015, 4, 7))]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "  help(date_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "o3IB6VxLgGYi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba2b858a-1188-4267-8aa2-efd6743eae24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------+----------------------+\n",
            "|date_sub(min(Date), 3)|date_add(max(Date), 3)|\n",
            "+----------------------+----------------------+\n",
            "|            2000-12-29|            2018-11-14|\n",
            "+----------------------+----------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "rc.select(date_sub(min(col('Date')),3),date_add(max(col('Date')),3)).show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gC1spS8LhBtJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}